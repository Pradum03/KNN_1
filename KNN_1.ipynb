{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a846937-b724-4c3c-8da1-84a7c60c681d",
   "metadata": {},
   "source": [
    "ASSIGNMENT: KNN-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec944626-0a43-4b0c-8d50-a32ec3de1a0d",
   "metadata": {},
   "source": [
    "1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0cdd9-28fa-453a-a240-277987cd4e29",
   "metadata": {},
   "source": [
    "\n",
    "KNN, or k-nearest neighbors, is a non-parametric machine learning algorithm that is used for classification and regression tasks. It is a simple but effective algorithm that can be used in a wide range of applications.\n",
    "\n",
    "In the KNN algorithm, a data point is classified by looking at the k-nearest data points in the training set. The \"k\" in KNN represents the number of nearest neighbors to consider. These nearest neighbors are chosen based on their distance to the data point being classified. The distance metric used can be Euclidean distance, Manhattan distance, or other measures depending on the problem.\n",
    "\n",
    "For classification tasks, the KNN algorithm assigns the most common class label among the k-nearest neighbors to the new data point. For regression tasks, the KNN algorithm returns the average or median value of the k-nearest neighbors as the predicted value for the new data point.\n",
    "\n",
    "One of the main advantages of the KNN algorithm is its simplicity and easy implementation. It also does not require any training or model building, making it a good choice for small datasets. However, the algorithm can be computationally expensive for large datasets, and the choice of the parameter k can affect the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb46739-5026-492b-86b5-b598005ed3f3",
   "metadata": {},
   "source": [
    "2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c0280-7e3d-4693-9104-b3c601e61534",
   "metadata": {},
   "source": [
    "Domain knowledge: In some cases, domain knowledge can help in selecting an appropriate value of k. For example, if we are classifying images of handwritten digits, we might expect that the nearest neighbors will be similar to the given digit.\n",
    "\n",
    "Grid search: One approach is to try different values of k and evaluate the performance of the algorithm on a validation set. We can use a grid search to select the best value of k by trying a range of values and choosing the one that gives the best performance.\n",
    "\n",
    "Cross-validation: Another approach is to use cross-validation to estimate the performance of the algorithm for different values of k. In k-fold cross-validation, we divide the data into k subsets, use k-1 subsets for training and the remaining subset for validation. We can repeat this process for different values of k and select the one that gives the best performance.\n",
    "\n",
    "Elbow method: The elbow method is a heuristic that can be used to select an optimal value of k. We plot the error rate (or any performance metric) for different values of k, and select the value of k at which the error rate starts to level off. This is called the \"elbow point\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9f36f-1667-49e9-8cb8-ac83dd6ac1ee",
   "metadata": {},
   "source": [
    "3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cceb267-e9e2-4a90-9221-e873fce12257",
   "metadata": {},
   "source": [
    "The KNN algorithm can be used for both classification and regression tasks. The main difference between KNN classifier and KNN regressor is the type of output that they produce.\n",
    "\n",
    "KNN Classifier:\n",
    "In KNN classifier, the objective is to predict the class of a given input by finding the K closest data points in the training set and assigning the most common class label among those K data points to the input. The output of KNN classifier is a discrete class label. KNN classifier is used for solving classification problems where the output variable is categorical.\n",
    "\n",
    "KNN Regressor:\n",
    "In KNN regressor, the objective is to predict the continuous value of the output variable by finding the K closest data points in the training set and calculating the average (or median) value of the output variable among those K data points. The output of KNN regressor is a continuous variable. KNN regressor is used for solving regression problems where the output variable is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d09177-aaf8-42d3-9e00-515e82bc359c",
   "metadata": {},
   "source": [
    "4.  How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d704b7-79c6-4af4-80d4-e846e7b8187a",
   "metadata": {},
   "source": [
    "KNN Classifier:\n",
    "\n",
    "Accuracy: the proportion of correctly classified instances over the total number of instances in the test dataset.\n",
    "\n",
    "Precision: the proportion of true positives over the total number of instances that are predicted as positive.\n",
    "\n",
    "Recall: the proportion of true positives over the total number of instances that are actually positive.\n",
    "\n",
    "F1 score: the harmonic mean of precision and recall, which gives a balanced measure of both metrics.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Mean Squared Error (MSE): the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "Root Mean Squared Error (RMSE): the square root of the MSE, which gives a measure of the average distance between the predicted and actual values.\n",
    "\n",
    "Mean Absolute Error (MAE): the average of the absolute differences between the predicted and actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c3821-20f2-4d57-8c82-b6029e1e1ad5",
   "metadata": {},
   "source": [
    "5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637029e-bc7f-43f4-92c3-4002c593310f",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a phenomenon that occurs in high-dimensional spaces, where the distance between any two points becomes increasingly large as the number of dimensions increases. This makes it difficult to find nearest neighbors accurately in KNN, because the concept of \"nearness\" becomes less meaningful in high-dimensional spaces.\n",
    "\n",
    "In KNN, the algorithm searches for the K nearest neighbors of a data point by calculating the distance between the data point and all other data points in the dataset. The distance metric used in KNN (such as Euclidean distance or Manhattan distance) is based on the geometric distance between two points in a space. As the number of dimensions increases, the volume of the space increases exponentially, and the distance between any two points also increases. This means that in high-dimensional spaces, most of the points are far away from any given point, and the K nearest neighbors are not necessarily representative of the true underlying structure of the data.\n",
    "\n",
    "The curse of dimensionality can be mitigated by reducing the dimensionality of the data using techniques such as Principal Component Analysis (PCA) or t-SNE, which can preserve the most important features of the data while reducing the number of dimensions. Another way to reduce the impact of the curse of dimensionality is to use distance metrics that are specifically designed for high-dimensional spaces, such as the cosine similarity metric. Finally, increasing the amount of data can also help reduce the impact of the curse of dimensionality, by providing more information to distinguish between the data points in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f40126-d38c-42dc-b8c6-361da97fbd48",
   "metadata": {},
   "source": [
    "6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11995c16-54d9-4292-a958-d687aecaae73",
   "metadata": {},
   "source": [
    "Deletion: One simple approach is to delete any data point that contains missing values. This can be done using either listwise deletion, where entire rows with missing values are removed, or pairwise deletion, where only the missing values in a particular pair of data points are ignored when calculating the distance between them. However, this approach can lead to loss of information, and may result in biased estimates if the missing data is not missing completely at random.\n",
    "\n",
    "Imputation: Another approach is to impute the missing values with some estimated values. This can be done using various methods such as mean imputation, where missing values are replaced with the mean of the available values for that feature, or regression imputation, where missing values are predicted using a regression model based on the other features. However, imputation methods can introduce noise into the data, and may not be appropriate if the missing data is not missing at random.\n",
    "\n",
    "KNN imputation: This method involves using the KNN algorithm itself to estimate the missing values. In this approach, the missing values for a particular data point are estimated based on the K nearest neighbors of that data point, using their available values for that feature. This approach can provide more accurate estimates of the missing values, but can also be computationally expensive and may not work well for data with high-dimensional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23cdb1-166c-445c-b3c9-b8b8988c5d82",
   "metadata": {},
   "source": [
    "7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for \n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327ee13-d122-41df-8bfb-d5c251ee0d87",
   "metadata": {},
   "source": [
    "\n",
    "KNN classifier and KNN regressor are two different variations of the K-Nearest Neighbors (KNN) algorithm, used for classification and regression problems, respectively. Here are some key differences between the two:\n",
    "\n",
    "Output: KNN classifier outputs the class label of the new data point based on the majority class label of its K nearest neighbors, while KNN regressor outputs the continuous numeric value based on the average of the K nearest neighbors.\n",
    "\n",
    "Data types: KNN classifier is used for categorical or nominal data types, where the class labels are discrete, while KNN regressor is used for numerical or continuous data types, where the target variable is continuous.\n",
    "\n",
    "Evaluation metrics: KNN classifier is evaluated using metrics such as accuracy, precision, recall, and F1 score, while KNN regressor is evaluated using metrics such as mean squared error, mean absolute error, and R-squared.\n",
    "\n",
    "Handling outliers: KNN classifier is robust to outliers, as it is based on the majority vote of the nearest neighbors, while KNN regressor is sensitive to outliers, as it is based on the average of the nearest neighbors.\n",
    "\n",
    "In terms of which one is better for which type of problem, it depends on the nature of the problem and the data. KNN classifier is suitable for problems where the target variable is categorical, such as image classification, text classification, or fraud detection. KNN regressor, on the other hand, is suitable for problems where the target variable is continuous, such as predicting house prices, stock prices, or weather forecasting.\n",
    "\n",
    "However, it is important to note that KNN algorithms can be sensitive to the choice of the K value and the distance metric, and may not perform well for high-dimensional or sparse data. Therefore, it is always recommended to try different algorithms and hyperparameters and evaluate their performance before making a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120801e-0db5-416e-ac25-d0d04bdaa49b",
   "metadata": {},
   "source": [
    "8.  What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, \n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f002a-fc56-475f-b1b7-2a7b54486a78",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "Simple to understand and easy to implement.\n",
    "\n",
    "KNN can work with any number of classes.\n",
    "\n",
    "KNN can work with any type of data.\n",
    "\n",
    "KNN is a non-parametric model, meaning it does not assume any underlying distribution of data.\n",
    "\n",
    "KNN can handle multi-modal data, where multiple classes are present in the same data set.\n",
    "\n",
    "\n",
    "KNN is effective when the decision boundary is irregular and complex.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "KNN can be sensitive to irrelevant or redundant features.\n",
    "\n",
    "KNN is computationally expensive for large datasets.\n",
    "\n",
    "KNN may suffer from the curse of dimensionality, where the distance metric becomes less meaningful as the number of dimensions increase.\n",
    "\n",
    "KNN is sensitive to the choice of the K value and the distance metric.\n",
    "\n",
    "KNN is prone to overfitting for small K values and underfitting for large K values.\n",
    "\n",
    "KNN may struggle with imbalanced data, where one class is significantly underrepresented.\n",
    "\n",
    "To address the weaknesses of the KNN algorithm, here are some strategies:\n",
    "\n",
    "Feature selection and dimensionality reduction techniques can be used to reduce irrelevant and redundant features.\n",
    "\n",
    "KNN can be made computationally efficient by using approximation methods such as k-d tree, ball tree or locality-sensitive hashing.\n",
    "\n",
    "\n",
    "Data normalization and standardization techniques can be used to avoid the curse of dimensionality.\n",
    "\n",
    "Cross-validation techniques can be used to select the optimal K value and distance metric.\n",
    "\n",
    "\n",
    "\n",
    "Ensemble methods such as bagging and boosting can be used to reduce the impact of overfitting and underfitting.\n",
    "\n",
    "Resampling techniques such as oversampling and undersampling can be used to balance the class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8743e-7f60-439b-a0df-55f69d6a9d50",
   "metadata": {},
   "source": [
    "9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7aaf9-77f7-41f8-8e7f-4eba5ac1452d",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm for finding the nearest neighbors.\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squared differences between the coordinates of two points. In other words, it is the length of the shortest path between two points in a straight line.\n",
    "\n",
    "Manhattan distance, also known as taxicab distance, is the distance between two points measured along the axes at right angles. It is calculated as the sum of the absolute differences between the coordinates of two points. In other words, it is the distance a taxi would have to travel on a grid-like city to get from one point to another, where the taxi can only travel along the grid lines and not diagonally.\n",
    "\n",
    "The main difference between Euclidean distance and Manhattan distance is that Euclidean distance takes into account the diagonal distance between two points, while Manhattan distance only considers the horizontal and vertical distance. This means that Euclidean distance may be more suitable for datasets with continuous features and where the distance between two points is important, while Manhattan distance may be more suitable for datasets with categorical features or where the distance between two points is based on the number of changes along the axes.\n",
    "\n",
    "In KNN algorithm, the choice of distance metric depends on the nature of the dataset and the problem at hand. Both Euclidean and Manhattan distances can be used in KNN algorithm and their effectiveness can be evaluated through cross-validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da94754-9031-4c86-8650-b62b43c27a9f",
   "metadata": {},
   "source": [
    "10.  What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835505e2-5def-49a7-b0da-7e785536929d",
   "metadata": {},
   "source": [
    "Feature scaling is an important preprocessing step in KNN algorithm. It refers to the transformation of the input features to a common scale in order to make them comparable.\n",
    "\n",
    "In KNN, the distance between two points is calculated based on the difference in their feature values. If the features have different scales, some features may dominate the distance calculation and overshadow the others. This can lead to biased and inaccurate results.\n",
    "\n",
    "For example, consider a dataset where one feature is measured in meters and another feature is measured in kilograms. The feature measured in meters will have a much larger scale than the feature measured in kilograms, which can result in the KNN algorithm focusing more on the distance between the values of the larger feature. In this case, feature scaling can help bring both features to a similar scale so that they can be equally weighted in the distance calculation.\n",
    "\n",
    "Therefore, feature scaling is important for KNN algorithm to ensure that the distance calculation is not affected by the scale of the features. Common methods of feature scaling include normalization and standardization. Normalization scales the features to a range between 0 and 1, while standardization scales the features to have zero mean and unit variance. The choice of scaling method depends on the distribution of the features and the nature of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8c2d7-4043-442d-8e72-1ca7b53fa0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
